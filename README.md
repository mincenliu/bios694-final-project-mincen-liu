# BIOS 694 Final Project: Knowledge Distillation in Practice
 
This project explores **knowledge distillation (KD)**â€”a model compression technique that transfers knowledge from a high-capacity teacher model to a smaller student modelâ€”using the MNIST handwritten digit classification dataset. All models and experiments were implemented in **R** using the `{torch}`, `{torchvision}`, and `{luz}` packages.

> **Author**: Mincen Liu  
> **Institution**: McGill University  
> **Date**: April 2025  

---

## ðŸ“‚ Repository Structure

â”œâ”€â”€ model/                                          # Saved .pt models  
â”‚   â”œâ”€â”€ mnist-cnn-teacher.pt                            # Teacher model (T = 3)  
â”‚   â”œâ”€â”€ mnist-student-kd-{Î±}.pt                         # Student models trained with KD (Î± = 0, 0.3, 0.7, 1)  
â”‚   â”œâ”€â”€ mnist-cnn-studentT.pt                           # Models trained without KD to illustrate temperature scaling (T = 1, 3, 10)  
â”œâ”€â”€ output/                                         # Rendered PDF reports  
â”‚   â”œâ”€â”€ KD/                                             # KD experiment reports  
â”‚   â”œâ”€â”€ softmax/                                        # Temperature scaling report and plot  
â”œâ”€â”€ BIOS694_Final_Project_Teacher_Model.Rmd         # Source code for training the teacher model  
â”œâ”€â”€ BIOS694_Final_Project_KD_{Î±}.Rmd                # Source code for training the student models with KD (Î± = 0, 0.3, 0.7, 1)  
â”œâ”€â”€ BIOS694_Final_Project_Student_Model_noKD.Rmd    # Source code for training models to illustrate temperature scaling (without KD)  
â”œâ”€â”€ BIOS694_Final_Project_Softmax_Plot.Rmd          # Source code for plotting the softmax distribution under different temperatures  

*Note:* The MNIST dataset will be automatically downloaded by the scripts when running the RMarkdown files.

---

## ðŸ“Œ Project Objectives

- Summarize and reproduce the results of Hinton et al. (2015) using MNIST.
- Investigate how softmax temperature scaling affects output distributions.
- Train student models with various Î± values in the KD loss:
  
  `L = Î± Â· L_CE + (1 âˆ’ Î±) Â· L_KL`

- Evaluate the trade-off between learning from hard targets (true labels) and soft targets (teacher output probabilities).

---

## ðŸ§  Models

- **TeacherNet**: CNN with ~1.63M parameters; trained with temperature ( T = 3 ).
- **StudentNet**: Compact CNN with ~102K parameters (6.3% of TeacherNet).

---

## ðŸ”¬ Experiments

- **KD with fixed temperature (T = 3)** and Î± âˆˆ {0, 0.3, 0.7, 1}.
- **Temperature scaling** on softmax output with T âˆˆ {1, 3, 10}.
- **Evaluation metric**: Test set accuracy.

### ðŸ“ˆ Results

| Î± value | Description                  | Test Accuracy (%) |
|--------:|------------------------------|-------------------|
| 0.0     | Only soft targets            | 98.10             |
| 0.3     | Small weight on hard targets | **98.34**         |
| 0.7     | Large weight on hard targets | 98.11             |
| 1.0     | Only hard targets (baseline) | 98.00             |
| â€”       | Teacher model                | 98.75             |

---

## ðŸš€ Getting Started

1. **Clone or download the repo**  
   git clone https://github.com/mincenliu/bios694-final-project-mincen-liu.git

2. **Open R and install dependencies**  
    install.packages("torch")
    install.packages("torchvision")
    install.packages("luz")
    install.packages("reshape2")
    install.packages("ggplot2")
    install.packages("dplyr")
    install.packages("tibble")
    install.packages("caret")
    install.packages("here")

3. **Run the scripts**  
    Open any of the .Rmd files to reproduce the KD experiments or plots.

---

## ðŸ“š References

```
Hinton et al. (2015). Distilling the Knowledge in a Neural Network. arXiv:1503.02531
Menon et al. (2021). A Statistical Perspective on Distillation. ICML
```
---

## ðŸ”§ Future Work

```
Compare R vs Python implementations of KD.

Extend to more complex datasets.

Explore feature-based or attention-based distillation techniques.
```