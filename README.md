# BIOS 694 Final Project: Knowledge Distillation in Practice
 
This project explores **knowledge distillation (KD)**—a model compression technique that transfers knowledge from a high-capacity teacher model to a smaller student model—using the MNIST handwritten digit classification dataset. All models and experiments were implemented in **R** using the `{torch}`, `{torchvision}`, and `{luz}` packages.

> **Author**: Mincen Liu  
> **Course**: BIOS 694  
> **Institution**: McGill University  
> **Date**: April 2025