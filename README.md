# BIOS 694 Final Project: Knowledge Distillation in Practice
 
This project explores **knowledge distillation (KD)**â€”a model compression technique that transfers knowledge from a high-capacity teacher model to a smaller student modelâ€”using the MNIST handwritten digit classification dataset. All models and experiments were implemented in **R** using the `{torch}`, `{torchvision}`, and `{luz}` packages.

> **Author**: Mincen Liu 
> **Institution**: McGill University  
> **Date**: April 2025

---

## ðŸ“‚ Repository Structure

â”œâ”€â”€ model/                                          # Saved .pt models
â”‚   â”œâ”€â”€ mnist-cnn-teacher.pt                            # Teacher model (T = 3)
â”‚   â”œâ”€â”€ mnist-student-kd-{Î±}.pt                         # Student models trained with KD (Î± = 0, 0.3, 0.7, 1)
â”‚   â”œâ”€â”€ mnist-cnn-studentT.pt                           # Models trained without KD to illustrate temperature scaling (T = 1, 3, 10)
â”œâ”€â”€ output/                                         # Rendered PDF reports
â”‚   â”œâ”€â”€ KD/                                             # KD experiment reports
â”‚   â”œâ”€â”€ softmax/                                        # Temperature scaling report and plot
â”œâ”€â”€ BIOS694_Final_Project_Teacher_Model.Rmd         # Source code for training the teacher model 
â”œâ”€â”€ BIOS694_Final_Project_KD_{Î±}.Rmd                # Source code for training the student models with KD (Î± = 0, 0.3, 0.7, 1)
â”œâ”€â”€ BIOS694_Final_Project_Student_Model_noKD.Rmd    # Source code for training models to illustrate temperature scaling (without KD)
â”œâ”€â”€ BIOS694_Final_Project_Softmax_Plot.Rmd          # Source code for plotting the softmax distribution under different temperatures

*Note:* The MNIST dataset will be automatically downloaded by the scripts when running the RMarkdown files.

---
